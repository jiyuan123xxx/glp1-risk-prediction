#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æè¯•éªŒå¼€å§‹å¹´ä»½åˆ†å¸ƒï¼ŒæŒ‰2021å¹´åˆ†å‰²æ•°æ®
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# é…ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def analyze_time_distribution():
    """åˆ†æè¯•éªŒå¼€å§‹å¹´ä»½åˆ†å¸ƒ"""
    
    # è¯»å–æ•°æ®
    data = pd.read_csv('processed_data/glp1_18clinical_features_with_labels_correct.csv')
    
    print('=== è¯•éªŒå¼€å§‹å¹´ä»½åˆ†å¸ƒåˆ†æ ===')
    print(f'æ•°æ®æ€»æ•°: {len(data)}')
    print(f'å¹´ä»½èŒƒå›´: {data["start_year"].min():.1f} - {data["start_year"].max():.1f}')
    print(f'å¹´ä»½ä¸­ä½æ•°: {data["start_year"].median():.1f}')
    print(f'å¹´ä»½å‡å€¼: {data["start_year"].mean():.1f}')
    
    # æŒ‰2021å¹´åˆ†å‰²
    before_2021 = data[data['start_year'] < 2021]
    after_2021 = data[data['start_year'] >= 2021]
    
    print(f'\n=== æŒ‰2021å¹´åˆ†å‰²ç»“æœ ===')
    print(f'2021å¹´ä¹‹å‰æ ·æœ¬æ•°: {len(before_2021)} ({len(before_2021)/len(data)*100:.1f}%)')
    print(f'2021å¹´åŠä¹‹åæ ·æœ¬æ•°: {len(after_2021)} ({len(after_2021)/len(data)*100:.1f}%)')
    
    # åˆ†ææ ‡ç­¾åˆ†å¸ƒ
    print(f'\n=== æ ‡ç­¾åˆ†å¸ƒåˆ†æ ===')
    print('æ€»ä½“æ ‡ç­¾åˆ†å¸ƒ:')
    print(data['label'].value_counts())
    print(f'é«˜é£é™©æ¯”ä¾‹: {data["label"].mean()*100:.2f}%')
    
    print(f'\n2021å¹´ä¹‹å‰æ ‡ç­¾åˆ†å¸ƒ:')
    print(before_2021['label'].value_counts())
    print(f'é«˜é£é™©æ¯”ä¾‹: {before_2021["label"].mean()*100:.2f}%')
    
    print(f'\n2021å¹´åŠä¹‹åæ ‡ç­¾åˆ†å¸ƒ:')
    print(after_2021['label'].value_counts())
    print(f'é«˜é£é™©æ¯”ä¾‹: {after_2021["label"].mean()*100:.2f}%')
    
    # å¹´ä»½åˆ†å¸ƒç›´æ–¹å›¾
    print(f'\n=== å¹´ä»½åˆ†å¸ƒè¯¦æƒ… ===')
    year_counts = data['start_year'].value_counts().sort_index()
    print('å¹´ä»½åˆ†å¸ƒ:')
    for year, count in year_counts.items():
        print(f'{year:.0f}: {count}ä¸ªè¯•éªŒ')
    
    # å¯è§†åŒ–å¹´ä»½åˆ†å¸ƒ
    plt.figure(figsize=(12, 6))
    
    # å¹´ä»½åˆ†å¸ƒç›´æ–¹å›¾
    plt.subplot(1, 2, 1)
    plt.hist(data['start_year'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    plt.axvline(2021, color='red', linestyle='--', linewidth=2, label='2021å¹´åˆ†å‰²çº¿')
    plt.xlabel('è¯•éªŒå¼€å§‹å¹´ä»½')
    plt.ylabel('è¯•éªŒæ•°é‡')
    plt.title('è¯•éªŒå¼€å§‹å¹´ä»½åˆ†å¸ƒ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # é«˜é£é™©è¯•éªŒå¹´ä»½åˆ†å¸ƒ
    plt.subplot(1, 2, 2)
    high_risk_data = data[data['label'] == 1]
    if len(high_risk_data) > 0:
        plt.hist(high_risk_data['start_year'], bins=20, alpha=0.7, color='red', edgecolor='black')
        plt.axvline(2021, color='red', linestyle='--', linewidth=2, label='2021å¹´åˆ†å‰²çº¿')
        plt.xlabel('è¯•éªŒå¼€å§‹å¹´ä»½')
        plt.ylabel('é«˜é£é™©è¯•éªŒæ•°é‡')
        plt.title('é«˜é£é™©è¯•éªŒå¹´ä»½åˆ†å¸ƒ')
        plt.legend()
        plt.grid(True, alpha=0.3)
    else:
        plt.text(0.5, 0.5, 'æ— é«˜é£é™©è¯•éªŒæ•°æ®', ha='center', va='center', transform=plt.gca().transAxes)
    
    plt.tight_layout()
    plt.savefig('results/time_distribution_analysis.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f'\nâœ… æ—¶é—´åˆ†å¸ƒåˆ†æå›¾å·²ä¿å­˜è‡³ results/time_distribution_analysis.png')
    
    return data, before_2021, after_2021

def create_time_split_model():
    """åˆ›å»ºåŸºäºæ—¶é—´åˆ†å‰²çš„æ¨¡å‹"""
    
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
    from sklearn.preprocessing import StandardScaler
    
    # è¯»å–æ•°æ®
    data = pd.read_csv('processed_data/glp1_18clinical_features_with_labels_correct.csv')
    
    # ç‰¹å¾åˆ—å’Œæ ‡ç­¾åˆ—
    feature_cols = [col for col in data.columns if col not in ['nct_id', 'label']]
    X = data[feature_cols]
    y = data['label']
    
    # æ–¹æ¡ˆ1: ä¼ ç»Ÿéšæœºåˆ†å‰²ï¼ˆä½œä¸ºåŸºå‡†ï¼‰
    X_train_rand, X_test_rand, y_train_rand, y_test_rand = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # æ–¹æ¡ˆ2: æŒ‰2021å¹´æ—¶é—´åˆ†å‰²
    # è®­ç»ƒé›†: 2021å¹´ä¹‹å‰çš„æ•°æ®
    # æµ‹è¯•é›†: 2021å¹´åŠä¹‹åçš„æ•°æ®
    train_mask = data['start_year'] < 2021
    test_mask = data['start_year'] >= 2021
    
    X_train_time = X[train_mask]
    X_test_time = X[test_mask]
    y_train_time = y[train_mask]
    y_test_time = y[test_mask]
    
    print(f'\n=== æ—¶é—´åˆ†å‰²æ–¹æ¡ˆè¯¦æƒ… ===')
    print(f'è®­ç»ƒé›†å¤§å°: {len(X_train_time)} ({len(X_train_time)/len(data)*100:.1f}%)')
    print(f'æµ‹è¯•é›†å¤§å°: {len(X_test_time)} ({len(X_test_time)/len(data)*100:.1f}%)')
    print(f'è®­ç»ƒé›†é«˜é£é™©æ¯”ä¾‹: {y_train_time.mean()*100:.2f}%')
    print(f'æµ‹è¯•é›†é«˜é£é™©æ¯”ä¾‹: {y_test_time.mean()*100:.2f}%')
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_rand_scaled = scaler.fit_transform(X_train_rand)
    X_test_rand_scaled = scaler.transform(X_test_rand)
    
    X_train_time_scaled = scaler.fit_transform(X_train_time)
    X_test_time_scaled = scaler.transform(X_test_time)
    
    # è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹
    lr_rand = LogisticRegression(class_weight='balanced', random_state=42)
    lr_time = LogisticRegression(class_weight='balanced', random_state=42)
    
    lr_rand.fit(X_train_rand_scaled, y_train_rand)
    lr_time.fit(X_train_time_scaled, y_train_time)
    
    # é¢„æµ‹
    y_pred_rand = lr_rand.predict(X_test_rand_scaled)
    y_pred_time = lr_time.predict(X_test_time_scaled)
    
    y_pred_proba_rand = lr_rand.predict_proba(X_test_rand_scaled)[:, 1]
    y_pred_proba_time = lr_time.predict_proba(X_test_time_scaled)[:, 1]
    
    # è¯„ä¼°æŒ‡æ ‡
    def evaluate_model(y_true, y_pred, y_pred_proba, model_name):
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        auc = roc_auc_score(y_true, y_pred_proba)
        
        return {
            'æ¨¡å‹': model_name,
            'å‡†ç¡®ç‡': accuracy,
            'ç²¾ç¡®ç‡': precision,
            'å¬å›ç‡': recall,
            'F1åˆ†æ•°': f1,
            'AUC': auc
        }
    
    results = []
    results.append(evaluate_model(y_test_rand, y_pred_rand, y_pred_proba_rand, 'éšæœºåˆ†å‰²'))
    results.append(evaluate_model(y_test_time, y_pred_time, y_pred_proba_time, 'æ—¶é—´åˆ†å‰²(2021)'))
    
    # è¾“å‡ºç»“æœ
    results_df = pd.DataFrame(results)
    print(f'\n=== æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ ===')
    print(results_df.round(4))
    
    # ä¿å­˜ç»“æœ
    results_df.to_csv('results/time_split_comparison.csv', index=False, encoding='utf-8-sig')
    print(f'âœ… æ—¶é—´åˆ†å‰²æ¯”è¾ƒç»“æœå·²ä¿å­˜è‡³ results/time_split_comparison.csv')
    
    return results_df

if __name__ == "__main__":
    print("ğŸ” å¼€å§‹åˆ†ææ—¶é—´åˆ†å‰²æ•ˆæœ...")
    
    # åˆ†ææ—¶é—´åˆ†å¸ƒ
    data, before_2021, after_2021 = analyze_time_distribution()
    
    # åˆ›å»ºæ—¶é—´åˆ†å‰²æ¨¡å‹
    results = create_time_split_model()
    
    print("\nğŸ‰ æ—¶é—´åˆ†å‰²åˆ†æå®Œæˆï¼")