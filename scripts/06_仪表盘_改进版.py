#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GLP-1 æ¨¡å‹æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿ï¼ˆå®Œæ•´ç‰ˆï¼šSHAP + LIME + PDPï¼‰
åŠŸèƒ½ï¼š
1. åŠ è½½æœ€ä½³æ¨¡å‹ï¼ˆé€»è¾‘å›å½’ï¼‰åŠæµ‹è¯•æ•°æ®
2. ç»˜åˆ¶ ROCã€PRã€æ··æ·†çŸ©é˜µã€æ¦‚ç‡åˆ†å¸ƒã€é˜ˆå€¼ä¼˜åŒ–æ›²çº¿
3. è¾“å‡ºç‰¹å¾é‡è¦æ€§ï¼ˆç³»æ•°ï¼‰å›¾
4. è¿›è¡Œ SHAP å…¨å±€è§£é‡Šï¼Œç”Ÿæˆæ€»ç»“å›¾
5. è¿›è¡Œ LIME å±€éƒ¨è§£é‡Šï¼Œç”Ÿæˆä¸€ä¸ªé«˜é£é™©æ ·æœ¬çš„ HTML è§£é‡Š
6. è¿›è¡Œ PDP åˆ†æï¼Œå±•ç¤ºå…³é”®ç‰¹å¾ä¸é¢„æµ‹æ¦‚ç‡çš„è¾¹é™…å…³ç³»
7. ç”Ÿæˆ Markdown æ€§èƒ½æŠ¥å‘Š
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
import sys
from datetime import datetime
from sklearn.metrics import (
    roc_curve, precision_recall_curve, auc, confusion_matrix,
    roc_auc_score, average_precision_score, f1_score,
    accuracy_score, precision_score, recall_score
)
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.inspection import PartialDependenceDisplay

# å¯é€‰å¯¼å…¥ SHAP å’Œ LIMEï¼Œè‹¥æœªå®‰è£…åˆ™è·³è¿‡
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("âš ï¸ SHAP æœªå®‰è£…ï¼Œè·³è¿‡ SHAP åˆ†æã€‚")

try:
    import lime
    import lime.lime_tabular
    LIME_AVAILABLE = True
except ImportError:
    LIME_AVAILABLE = False
    print("âš ï¸ LIME æœªå®‰è£…ï¼Œè·³è¿‡ LIME åˆ†æã€‚")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# é…ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ç‰¹å¾åç§°ä¸­æ–‡æ˜ å°„
FEATURE_NAMES_CN = {
    'enrollment_log': 'æ³¨å†Œäººæ•°å¯¹æ•°å˜æ¢ï¼ˆè¯•éªŒè§„æ¨¡ä¸ç»Ÿè®¡æ•ˆåŠ›ç‰¹å¾ï¼‰',
    'start_year': 'è¯•éªŒå¼€å§‹å¹´ä»½ï¼ˆè¯ç‰©ç ”å‘æ—¶ä»£ä¸é‡Œç¨‹ç¢‘ç‰¹å¾ï¼‰',
    'pre_semaglutide_era': 'å¸ç¾æ ¼é²è‚½ä¸Šå¸‚å‰æ—¶ä»£ï¼ˆè¯ç‰©ç ”å‘æ—¶ä»£ä¸é‡Œç¨‹ç¢‘ç‰¹å¾ï¼‰',
    'post_semaglutide_era': 'å¸ç¾æ ¼é²è‚½ä¸Šå¸‚åæ—¶ä»£ï¼ˆè¯ç‰©ç ”å‘æ—¶ä»£ä¸é‡Œç¨‹ç¢‘ç‰¹å¾ï¼‰',
    'phase_Unknown': 'è¯•éªŒé˜¶æ®µæœªçŸ¥ï¼ˆè¯•éªŒé˜¶æ®µä¸ç›‘ç®¡é£é™©ç‰¹å¾ï¼‰',
    'phase_PHASE4': 'IVæœŸä¸Šå¸‚åç ”ç©¶ï¼ˆè¯•éªŒé˜¶æ®µä¸ç›‘ç®¡é£é™©ç‰¹å¾ï¼‰',
    'is_obesity': 'è‚¥èƒ–ç›¸å…³è¯•éªŒï¼ˆé€‚åº”ç—‡ä¸ç›®æ ‡äººç¾¤é£é™©ç‰¹å¾ï¼‰',
    'is_t2d': '2å‹ç³–å°¿ç—…è¯•éªŒï¼ˆé€‚åº”ç—‡ä¸ç›®æ ‡äººç¾¤é£é™©ç‰¹å¾ï¼‰',
    'is_weight_loss': 'å‡é‡ä¸ºä¸»è¦ç»ˆç‚¹çš„è¯•éªŒï¼ˆé€‚åº”ç—‡ä¸ç›®æ ‡äººç¾¤é£é™©ç‰¹å¾ï¼‰',
    'exc_count': 'æ’é™¤æ ‡å‡†æ•°é‡ï¼ˆå…¥æ’æ ‡å‡†ä¸æ‚£è€…é€‰æ‹©ç‰¹å¾ï¼‰',
    'criteria_total_len': 'å…¥æ’æ ‡å‡†æ€»å­—ç¬¦æ•°ï¼ˆå…¥æ’æ ‡å‡†ä¸æ‚£è€…é€‰æ‹©ç‰¹å¾ï¼‰',
    'mentions_bmi': 'æåŠBMIï¼ˆå®‰å…¨æ€§æ–‡æœ¬ä¿¡å·å¼ºåº¦ç‰¹å¾ï¼‰',
    'mentions_contraindication': 'æåŠç¦å¿Œç—‡ï¼ˆå®‰å…¨æ€§æ–‡æœ¬ä¿¡å·å¼ºåº¦ç‰¹å¾ï¼‰',
    'mentions_renal_cutoff': 'æåŠè‚¾åŠŸèƒ½é˜ˆå€¼ï¼ˆå®‰å…¨æ€§æ–‡æœ¬ä¿¡å·å¼ºåº¦ç‰¹å¾ï¼‰',
    'high_risk_term_count': 'é«˜é£é™©æœ¯è¯­è®¡æ•°ï¼ˆå®‰å…¨æ€§æ–‡æœ¬ä¿¡å·å¼ºåº¦ç‰¹å¾ï¼‰',
    'risk_ratio': 'é£é™©æ¯”ç‡ï¼ˆå®‰å…¨æ€§æ–‡æœ¬ä¿¡å·å¼ºåº¦ç‰¹å¾ï¼‰',
    'year_x_enrollment': 'å¹´ä»½ Ã— æ³¨å†Œäººæ•°å¯¹æ•°ï¼ˆäº¤äº’ç‰¹å¾ï¼‰',
    'enrollment_log_x_phase_Unknown': 'æ³¨å†Œäººæ•°å¯¹æ•° Ã— é˜¶æ®µæœªçŸ¥ï¼ˆäº¤äº’ç‰¹å¾ï¼‰'
}

# ==================== é…ç½®å‚æ•°ï¼ˆä¸ 05_é›†æˆå­¦ä¹ .py ä¿æŒä¸€è‡´ï¼‰ ====================
RANDOM_STATE = 42
TEST_SIZE = 0.2
USE_TIME_SPLIT = True           # æ˜¯å¦æŒ‰å¹´ä»½åˆ’åˆ†
TIME_THRESHOLD = 2018            # ç”¨ 2018 å¹´åŠä»¥åä½œä¸ºæµ‹è¯•é›†

# è·¯å¾„è®¾ç½®
PROCESSED_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data', 'processed')
MODELS_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'models')
RESULTS_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'results')
VISUALIZATIONS_DIR = os.path.join(RESULTS_DIR, 'visualizations')
REPORTS_DIR = os.path.join(RESULTS_DIR, 'reports')
os.makedirs(VISUALIZATIONS_DIR, exist_ok=True)
os.makedirs(REPORTS_DIR, exist_ok=True)


def load_full_data():
    """åŠ è½½å®Œæ•´çš„å¸¦æ ‡ç­¾ç‰¹å¾æ•°æ®"""
    data_path = os.path.join(PROCESSED_DIR, 'glp1_18clinical_features_with_labels_correct.csv')
    df = pd.read_csv(data_path)
    feature_cols = [c for c in df.columns if c not in ['nct_id', 'label']]
    X = df[feature_cols].values
    y = df['label'].values
    return X, y, feature_cols, df


def split_data(X, y, df, use_time_split, time_threshold):
    """ä¸ 05_é›†æˆå­¦ä¹ .py ç›¸åŒçš„åˆ’åˆ†é€»è¾‘"""
    if use_time_split:
        if 'start_year' not in df.columns:
            raise ValueError("æŒ‰æ—¶é—´åˆ’åˆ†éœ€è¦ start_year åˆ—ï¼Œè¯·æ£€æŸ¥æ•°æ®ã€‚")
        train_mask = df['start_year'] < time_threshold
        test_mask = df['start_year'] >= time_threshold
        X_train, X_test = X[train_mask], X[test_mask]
        y_train, y_test = y[train_mask], y[test_mask]
        print(f"æ—¶é—´åˆ’åˆ†ï¼šè®­ç»ƒé›† {train_mask.sum()} æ ·æœ¬ (å¹´ä»½ < {time_threshold})ï¼Œæµ‹è¯•é›† {test_mask.sum()} æ ·æœ¬")
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_STATE
        )
        print(f"éšæœºåˆ†å±‚åˆ’åˆ†ï¼šè®­ç»ƒé›† {len(X_train)} æ ·æœ¬ï¼Œæµ‹è¯•é›† {len(X_test)} æ ·æœ¬")
    return X_train, X_test, y_train, y_test


def load_model_and_scaler():
    """åŠ è½½æ ‡å‡†åŒ–å™¨å’Œæœ€ä½³æ¨¡å‹ pipeline"""
    scaler = joblib.load(os.path.join(MODELS_DIR, 'standard_scaler.pkl'))
    model_pipeline = joblib.load(os.path.join(MODELS_DIR, 'best_model_pipeline.pkl'))
    feature_names = pd.read_csv(os.path.join(MODELS_DIR, 'feature_names.csv'), header=None).squeeze().tolist()
    return scaler, model_pipeline, feature_names


def calculate_metrics(y_true, y_pred, y_pred_proba):
    """è®¡ç®—å¸¸ç”¨æŒ‡æ ‡"""
    return {
        'auc': roc_auc_score(y_true, y_pred_proba),
        'pr_auc': average_precision_score(y_true, y_pred_proba),
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, zero_division=0),
        'recall': recall_score(y_true, y_pred, zero_division=0),
        'f1': f1_score(y_true, y_pred, zero_division=0)
    }


def plot_roc_curve(y_true, y_pred_proba, ax=None):
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    ax.plot(fpr, tpr, lw=2, label=f'AUC = {roc_auc:.4f}')
    ax.plot([0, 1], [0, 1], 'k--', label='éšæœºåˆ†ç±»å™¨')
    ax.set_xlabel('å‡æ­£ç‡ (FPR)', fontsize=12)
    ax.set_ylabel('çœŸæ­£ç‡ (TPR)', fontsize=12)
    ax.set_title('ROCæ›²çº¿ - æ¨¡å‹åŒºåˆ†èƒ½åŠ›è¯„ä¼°', fontsize=14, fontweight='bold')
    ax.legend(loc='lower right', fontsize=10)
    ax.grid(True, alpha=0.3)
    return roc_auc


def plot_pr_curve(y_true, y_pred_proba, ax=None):
    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)
    pr_auc = auc(recall, precision)
    ax.plot(recall, precision, lw=2, label=f'PR AUC = {pr_auc:.4f}')
    ax.set_xlabel('å¬å›ç‡ (Recall)', fontsize=12)
    ax.set_ylabel('ç²¾ç¡®ç‡ (Precision)', fontsize=12)
    ax.set_title('ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿ - ä¸å¹³è¡¡æ•°æ®æ€§èƒ½è¯„ä¼°', fontsize=14, fontweight='bold')
    ax.legend(loc='upper right', fontsize=10)
    ax.grid(True, alpha=0.3)
    return pr_auc


def plot_confusion_matrix(y_true, y_pred, ax=None):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['é¢„æµ‹ä½é£é™©', 'é¢„æµ‹é«˜é£é™©'],
                yticklabels=['å®é™…ä½é£é™©', 'å®é™…é«˜é£é™©'],
                ax=ax, cbar=False, annot_kws={"size": 12})
    ax.set_title('æ··æ·†çŸ©é˜µ - åˆ†ç±»ç»“æœå¯è§†åŒ–', fontsize=14, fontweight='bold')
    ax.set_xlabel('é¢„æµ‹ç±»åˆ«', fontsize=12)
    ax.set_ylabel('å®é™…ç±»åˆ«', fontsize=12)


def plot_probability_distribution(y_true, y_pred_proba, ax=None):
    ax.hist(y_pred_proba[y_true == 0], bins=30, alpha=0.5, label='ä½é£é™©', color='blue', density=True)
    ax.hist(y_pred_proba[y_true == 1], bins=30, alpha=0.5, label='é«˜é£é™©', color='red', density=True)
    ax.set_xlabel('é¢„æµ‹æ¦‚ç‡', fontsize=12)
    ax.set_ylabel('å¯†åº¦', fontsize=12)
    ax.set_title('é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ - é«˜é£é™©ä¸ä½é£é™©æ ·æœ¬åŒºåˆ†åº¦', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)


def plot_threshold_optimization(y_true, y_pred_proba, ax=None):
    thresholds = np.linspace(0.1, 0.9, 17)
    accs, f1s = [], []
    for thresh in thresholds:
        y_pred_t = (y_pred_proba >= thresh).astype(int)
        accs.append(accuracy_score(y_true, y_pred_t))
        f1s.append(f1_score(y_true, y_pred_t, zero_division=0))
    ax.plot(thresholds, accs, 'o-', label='å‡†ç¡®ç‡', linewidth=2)
    ax.plot(thresholds, f1s, 's-', label='F1åˆ†æ•°', linewidth=2)
    ax.set_xlabel('åˆ†ç±»é˜ˆå€¼', fontsize=12)
    ax.set_ylabel('æ€§èƒ½åˆ†æ•°', fontsize=12)
    ax.set_title('é˜ˆå€¼ä¼˜åŒ–æ›²çº¿ - å¹³è¡¡å‡†ç¡®ç‡ä¸F1åˆ†æ•°', fontsize=14, fontweight='bold')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)
    best_idx = np.argmax(f1s)
    best_thresh = thresholds[best_idx]
    best_f1 = f1s[best_idx]
    ax.axvline(best_thresh, color='red', linestyle='--', linewidth=2, label=f'æœ€ä½³F1é˜ˆå€¼={best_thresh:.2f}')
    ax.legend(fontsize=10)
    return best_thresh, best_f1


def plot_feature_importance_logistic(model, feature_names, ax=None):
    """é€»è¾‘å›å½’ç³»æ•°å›¾ï¼ˆç»å¯¹å€¼æ’åºï¼Œæ ‡æ³¨æ­£è´Ÿï¼‰"""
    clf = model.steps[-1][1]  # pipelineæœ€åä¸€æ­¥
    if not hasattr(clf, 'coef_'):
        ax.text(0.5, 0.5, 'æ¨¡å‹æ— ç³»æ•°ä¿¡æ¯', ha='center', va='center')
        return
    coef = clf.coef_[0]
    indices = np.argsort(np.abs(coef))[::-1]
    top_n = min(18, len(feature_names))
    top_indices = indices[:top_n]
    top_coef = coef[top_indices]
    top_names = [FEATURE_NAMES_CN.get(feature_names[i], feature_names[i]) for i in top_indices]
    colors = ['red' if c > 0 else 'green' for c in top_coef]
    ax.barh(range(top_n), np.abs(top_coef), color=colors, alpha=0.7)
    ax.set_yticks(range(top_n))
    ax.set_yticklabels(top_names, fontsize=9)
    ax.set_xlabel('ç³»æ•°ç»å¯¹å€¼', fontsize=12)
    ax.set_title('ç‰¹å¾é‡è¦æ€§åˆ†æï¼ˆé€»è¾‘å›å½’ç³»æ•°ï¼‰\nçº¢è‰²ï¼šæ­£ç›¸å…³ï¼ˆå¢åŠ é£é™©ï¼‰ï¼Œç»¿è‰²ï¼šè´Ÿç›¸å…³ï¼ˆé™ä½é£é™©ï¼‰', 
                 fontsize=14, fontweight='bold')
    ax.invert_yaxis()
    ax.grid(axis='x', alpha=0.3)


def shap_analysis(model, X_train, X_test, feature_names, save_dir):
    """SHAP å…¨å±€è§£é‡Šï¼šç”Ÿæˆ summary plot"""
    if not SHAP_AVAILABLE:
        print("âš ï¸ SHAP æœªå®‰è£…ï¼Œè·³è¿‡ SHAP åˆ†æã€‚")
        return None
    try:
        # ä» pipeline ä¸­æå–åˆ†ç±»å™¨ï¼ˆæœ€åä¸€æ­¥ï¼‰
        clf = model.steps[-1][1]
        # åˆ›å»º LinearExplainerï¼ˆéœ€è¦è®­ç»ƒæ•°æ®ä½œä¸ºèƒŒæ™¯ï¼‰
        explainer = shap.LinearExplainer(clf, X_train)
        shap_values = explainer.shap_values(X_test)

        # ä½¿ç”¨ä¸­æ–‡ç‰¹å¾åç§°
        feature_names_cn = [FEATURE_NAMES_CN.get(name, name) for name in feature_names]

        # ç»˜åˆ¶ summary plot
        plt.figure(figsize=(12, 10))
        shap.summary_plot(shap_values, X_test, feature_names=feature_names_cn, show=False)
        plt.title('SHAP ç‰¹å¾é‡è¦æ€§æ€»ç»“ - å…¨å±€å¯è§£é‡Šæ€§åˆ†æï¼ˆåŸºäºæµ‹è¯•é›†ï¼‰', fontsize=16, fontweight='bold')
        plt.tight_layout()
        save_path = os.path.join(save_dir, 'shap_summary_dashboard.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"âœ… SHAP æ€»ç»“å›¾å·²ä¿å­˜è‡³ {save_path}")
        return save_path
    except Exception as e:
        print(f"âŒ SHAP åˆ†æå¤±è´¥: {e}")
        return None


def lime_analysis(model, X_train, X_test, y_test, feature_names, save_dir):
    """LIME å±€éƒ¨è§£é‡Šï¼šé€‰æ‹©ä¸€ä¸ªé«˜é£é™©æµ‹è¯•æ ·æœ¬ç”Ÿæˆè§£é‡Š"""
    if not LIME_AVAILABLE:
        print("âš ï¸ LIME æœªå®‰è£…ï¼Œè·³è¿‡ LIME åˆ†æã€‚")
        return None
    try:
        # æ‰¾å‡ºé«˜é£é™©æ ·æœ¬ï¼ˆå®é™…æ ‡ç­¾ä¸º1ï¼‰ä¸”æ¨¡å‹é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬ï¼ˆå¯é€‰ï¼‰
        high_risk_idx = np.where(y_test == 1)[0]
        if len(high_risk_idx) == 0:
            print("âš ï¸ æµ‹è¯•é›†ä¸­æ— é«˜é£é™©æ ·æœ¬ï¼Œè·³è¿‡ LIME åˆ†æã€‚")
            return None
        # é€‰æ‹©ç¬¬ä¸€ä¸ªé«˜é£é™©æ ·æœ¬
        idx = high_risk_idx[0]
        instance = X_test[idx].reshape(1, -1)

        # è·å–é¢„æµ‹å‡½æ•°ï¼ˆpipeline çš„ predict_probaï¼‰
        predict_fn = model.predict_proba

        # ä½¿ç”¨ä¸­æ–‡ç‰¹å¾åç§°
        feature_names_cn = [FEATURE_NAMES_CN.get(name, name) for name in feature_names]

        # åˆ›å»º LIME è§£é‡Šå™¨ï¼ˆä½¿ç”¨è®­ç»ƒæ•°æ®æ‹Ÿåˆåˆ†å¸ƒï¼‰
        explainer = lime.lime_tabular.LimeTabularExplainer(
            training_data=X_train,
            feature_names=feature_names_cn,
            class_names=['ä½é£é™©', 'é«˜é£é™©'],
            mode='classification',
            discretize_continuous=True
        )

        # è§£é‡Šå•ä¸ªæ ·æœ¬
        exp = explainer.explain_instance(
            data_row=instance.flatten(),
            predict_fn=predict_fn,
            num_features=10,
            top_labels=1
        )

        # ä¿å­˜ä¸º HTML æ–‡ä»¶
        save_path = os.path.join(save_dir, 'lime_explanation.html')
        exp.save_to_file(save_path)
        print(f"âœ… LIME è§£é‡Šå·²ä¿å­˜è‡³ {save_path}")

        # åŒæ—¶ç”Ÿæˆä¸€ä¸ªç®€å•çš„æ–‡æœ¬æè¿°
        print("\nğŸ“‹ LIME è§£é‡Šæ‘˜è¦ï¼ˆé«˜é£é™©æ ·æœ¬å±€éƒ¨è§£é‡Šï¼‰ï¼š")
        for feat, weight in exp.as_list(label=1):
            print(f"  {feat}: {weight:.4f}")
        return save_path
    except Exception as e:
        print(f"âŒ LIME åˆ†æå¤±è´¥: {e}")
        return None


def plot_pdp_analysis(model, X_train, feature_names, save_dir):
    """PDP åˆ†æï¼šå±•ç¤ºå…³é”®ç‰¹å¾ä¸é¢„æµ‹æ¦‚ç‡çš„è¾¹é™…å…³ç³»"""
    # é€‰æ‹©6ä¸ªå…³é”®ç‰¹å¾ï¼ˆå¯æ ¹æ® SHAP é‡è¦æ€§æˆ–ä¸´åºŠæ„ä¹‰è°ƒæ•´ï¼‰
    pdp_features = [
        ('exc_count', 'æ’é™¤æ ‡å‡†æ•°é‡ï¼ˆå…¥æ’æ ‡å‡†ä¸æ‚£è€…é€‰æ‹©ç‰¹å¾ï¼‰'),
        ('enrollment_log', 'æ³¨å†Œäººæ•°å¯¹æ•°å˜æ¢ï¼ˆè¯•éªŒè§„æ¨¡ä¸ç»Ÿè®¡æ•ˆåŠ›ç‰¹å¾ï¼‰'),
        ('risk_ratio', 'é£é™©æ¯”ç‡ï¼ˆå®‰å…¨æ€§æ–‡æœ¬ä¿¡å·å¼ºåº¦ç‰¹å¾ï¼‰'),
        ('phase_Unknown', 'è¯•éªŒé˜¶æ®µæœªçŸ¥ï¼ˆè¯•éªŒé˜¶æ®µä¸ç›‘ç®¡é£é™©ç‰¹å¾ï¼‰'),
        ('mentions_contraindication', 'æåŠç¦å¿Œç—‡ï¼ˆå®‰å…¨æ€§æ–‡æœ¬ä¿¡å·å¼ºåº¦ç‰¹å¾ï¼‰'),
        ('mentions_renal_cutoff', 'æåŠè‚¾åŠŸèƒ½é˜ˆå€¼ï¼ˆå®‰å…¨æ€§æ–‡æœ¬ä¿¡å·å¼ºåº¦ç‰¹å¾ï¼‰')
    ]

    fig, axes = plt.subplots(2, 3, figsize=(20, 14))
    fig.suptitle('éƒ¨åˆ†ä¾èµ–å›¾ (PDP) - å…³é”®ç‰¹å¾ä¸é«˜é£é™©æ¦‚ç‡çš„è¾¹é™…å…³ç³»åˆ†æ', fontsize=18, fontweight='bold')

    for idx, (feat_name, feat_label) in enumerate(pdp_features):
        ax = axes[idx // 3, idx % 3]
        try:
            feat_idx = feature_names.index(feat_name)
            # ä½¿ç”¨ä¸­æ–‡ç‰¹å¾åç§°
            feature_names_cn = [FEATURE_NAMES_CN.get(name, name) for name in feature_names]
            
            # ç»˜åˆ¶ PDP
            display = PartialDependenceDisplay.from_estimator(
                model, X_train, [feat_idx],
                feature_names=feature_names_cn,
                ax=ax, grid_resolution=50,
                kind='average'
            )
            ax.set_xlabel(feat_label, fontsize=12)
            ax.set_ylabel('é¢„æµ‹é«˜é£é™©æ¦‚ç‡', fontsize=12)
            ax.set_title(f'{feat_label}', fontsize=14, fontweight='bold')
            ax.grid(True, alpha=0.3)
            ax.tick_params(axis='both', which='major', labelsize=10)
        except ValueError:
            ax.text(0.5, 0.5, f'ç‰¹å¾ {feat_name} ä¸å­˜åœ¨', ha='center', va='center', fontsize=12)
            ax.set_title(feat_label, fontsize=14, fontweight='bold')

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    save_path = os.path.join(save_dir, 'pdp_analysis_dashboard.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… PDP åˆ†æå›¾å·²ä¿å­˜è‡³ {save_path}")
    return save_path


def generate_report(metrics, best_thresh, best_f1, y_true, y_pred,
                    shap_path=None, lime_path=None, pdp_path=None):
    cm = confusion_matrix(y_true, y_pred)
    report = f"""# GLP-1 ä¸´åºŠè¯•éªŒé£é™©é¢„æµ‹æ¨¡å‹æ€§èƒ½æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ•°æ®é›†æ¦‚è§ˆ
- æµ‹è¯•æ ·æœ¬æ€»æ•°: {len(y_true)}
- é«˜é£é™©æ ·æœ¬æ•°: {np.sum(y_true)} ({np.mean(y_true)*100:.2f}%)
- ä½é£é™©æ ·æœ¬æ•°: {len(y_true)-np.sum(y_true)} ({(1-np.mean(y_true))*100:.2f}%)

## æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
| æŒ‡æ ‡ | å€¼ |
|------|-----|
| AUC | {metrics['auc']:.4f} |
| PR-AUC | {metrics['pr_auc']:.4f} |
| å‡†ç¡®ç‡ | {metrics['accuracy']:.4f} |
| ç²¾ç¡®ç‡ | {metrics['precision']:.4f} |
| å¬å›ç‡ | {metrics['recall']:.4f} |
| F1åˆ†æ•° | {metrics['f1']:.4f} |

## é˜ˆå€¼ä¼˜åŒ–
- æœ€ä¼˜é˜ˆå€¼ï¼ˆåŸºäºF1ï¼‰: {best_thresh:.4f}
- å¯¹åº”F1åˆ†æ•°: {best_f1:.4f}

## æ··æ·†çŸ©é˜µï¼ˆé»˜è®¤é˜ˆå€¼0.5ï¼‰
```
         é¢„æµ‹ä½é£é™©  é¢„æµ‹é«˜é£é™©
å®é™…ä½é£é™©   {cm[0,0]:6d}      {cm[0,1]:6d}
å®é™…é«˜é£é™©   {cm[1,0]:6d}      {cm[1,1]:6d}
```

## å¯è§£é‡Šæ€§åˆ†æ
"""
    if shap_path:
        report += f"- SHAP å…¨å±€è§£é‡Šå›¾ï¼š`{shap_path}`\n"
    else:
        report += "- SHAP åˆ†ææœªæ‰§è¡Œæˆ–å¤±è´¥\n"
    if lime_path:
        report += f"- LIME å±€éƒ¨è§£é‡Šï¼ˆé«˜é£é™©æ ·æœ¬ï¼‰ï¼š`{lime_path}`\n"
    else:
        report += "- LIME åˆ†ææœªæ‰§è¡Œæˆ–å¤±è´¥\n"
    if pdp_path:
        report += f"- PDP è¾¹é™…æ•ˆåº”åˆ†æå›¾ï¼š`{pdp_path}`\n"
    else:
        report += "- PDP åˆ†ææœªæ‰§è¡Œæˆ–å¤±è´¥\n"

    report += f"""
## è§£é‡Šä¸å»ºè®®
- AUC = {metrics['auc']:.3f} è¡¨æ˜æ¨¡å‹å…·æœ‰ä¸€å®šçš„åŒºåˆ†èƒ½åŠ›ã€‚
- å¬å›ç‡ = {metrics['recall']:.3f} è¡¨ç¤ºæ¨¡å‹èƒ½è¯†åˆ« {metrics['recall']*100:.1f}% çš„å®é™…é«˜é£é™©è¯•éªŒã€‚
- ç²¾ç¡®ç‡åä½ï¼Œæç¤ºå‡é˜³æ€§è¾ƒå¤šï¼Œå¯é€šè¿‡æé«˜é˜ˆå€¼ç‰ºç‰²éƒ¨åˆ†å¬å›ç‡æ¢å–ç²¾ç¡®ç‡ã€‚
- ç±»åˆ«æåº¦ä¸å¹³è¡¡ï¼ˆé«˜é£é™©ä»… {np.mean(y_true)*100:.2f}%ï¼‰ï¼Œå»ºè®®æŒç»­å…³æ³¨å¬å›ç‡è€Œéå‡†ç¡®ç‡ã€‚
"""
    return report


def main():
    print("="*60)
    print("GLP-1 æ¨¡å‹æ€§èƒ½ç›‘æ§ä»ªè¡¨æ¿ï¼ˆå®Œæ•´ç‰ˆï¼šSHAP + LIME + PDPï¼‰")
    print("="*60)

    # 1. åŠ è½½å®Œæ•´æ•°æ®å¹¶é‡æ–°åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†ï¼ˆç”¨äº SHAP èƒŒæ™¯å’Œ LIME è®­ç»ƒæ•°æ®ï¼‰
    print("\nğŸ“¦ åŠ è½½å®Œæ•´æ•°æ®å¹¶åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†...")
    X, y, feature_names, df = load_full_data()
    X_train, X_test, y_train, y_test = split_data(X, y, df, USE_TIME_SPLIT, TIME_THRESHOLD)

    # 2. æ ‡å‡†åŒ–ï¼ˆé‡æ–°æ‹Ÿåˆè®­ç»ƒé›†ï¼‰
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 3. åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹
    print("\nğŸ¤– åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹...")
    scaler_saved, model_pipeline, feature_names_saved = load_model_and_scaler()
    # éªŒè¯ç‰¹å¾åæ˜¯å¦ä¸€è‡´ï¼ˆæ£€æŸ¥é›†åˆæ˜¯å¦ç›¸åŒï¼Œå¿½ç•¥é¡ºåºï¼‰
    if set(feature_names) != set(feature_names_saved):
        print("âš ï¸ ç‰¹å¾åç§°ä¸ä¸€è‡´ï¼Œä½†ç»§ç»­æ‰§è¡Œ...")
        print(f"  æ•°æ®ç‰¹å¾: {sorted(feature_names)}")
        print(f"  æ¨¡å‹ç‰¹å¾: {sorted(feature_names_saved)}")

    # 4. å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
    y_pred_proba = model_pipeline.predict_proba(X_test)[:, 1]
    y_pred = model_pipeline.predict(X_test)

    # 5. è®¡ç®—æŒ‡æ ‡
    metrics = calculate_metrics(y_test, y_pred, y_pred_proba)

    # 6. ç»˜åˆ¶ä»ªè¡¨æ¿
    print("\nğŸ“Š ç»˜åˆ¶æ€§èƒ½ä»ªè¡¨æ¿...")
    fig = plt.figure(figsize=(20, 14))
    fig.suptitle('GLP-1 ä¸´åºŠè¯•éªŒé£é™©é¢„æµ‹æ¨¡å‹æ€§èƒ½ä»ªè¡¨æ¿ - ç»¼åˆæ€§èƒ½è¯„ä¼°ä¸å¯è§£é‡Šæ€§åˆ†æ', fontsize=20, fontweight='bold')

    ax1 = plt.subplot(3, 3, 1)
    plot_roc_curve(y_test, y_pred_proba, ax1)

    ax2 = plt.subplot(3, 3, 2)
    plot_pr_curve(y_test, y_pred_proba, ax2)

    ax3 = plt.subplot(3, 3, 3)
    plot_confusion_matrix(y_test, y_pred, ax3)

    ax4 = plt.subplot(3, 3, 4)
    plot_probability_distribution(y_test, y_pred_proba, ax4)

    ax5 = plt.subplot(3, 3, 5)
    best_thresh, best_f1 = plot_threshold_optimization(y_test, y_pred_proba, ax5)

    ax6 = plt.subplot(3, 3, 6)
    plot_feature_importance_logistic(model_pipeline, feature_names, ax6)

    ax7 = plt.subplot(3, 3, (7, 9))
    ax7.axis('off')
    summary = f"""
    ğŸ“Š æ¨¡å‹æ€§èƒ½æ€»ç»“ï¼ˆæµ‹è¯•é›†ï¼‰
    ========================
    
    ğŸ¯ ä¸»è¦æŒ‡æ ‡
    AUC: {metrics['auc']:.4f} | PR-AUC: {metrics['pr_auc']:.4f}
    å‡†ç¡®ç‡: {metrics['accuracy']:.4f} | å¬å›ç‡: {metrics['recall']:.4f}
    ç²¾ç¡®ç‡: {metrics['precision']:.4f} | F1: {metrics['f1']:.4f}
    
    âš™ï¸ é˜ˆå€¼ä¼˜åŒ–
    æœ€ä¼˜é˜ˆå€¼: {best_thresh:.4f} (F1={best_f1:.4f})
    
    ğŸ“ˆ æ•°æ®åˆ†å¸ƒ
    é«˜é£é™©æ ·æœ¬æ•°: {np.sum(y_test)} / {len(y_test)} ({np.mean(y_test)*100:.2f}%)
    ä½é£é™©æ ·æœ¬æ•°: {len(y_test)-np.sum(y_test)} / {len(y_test)} ({(1-np.mean(y_test))*100:.2f}%)
    
    ğŸ’¡ æ¨¡å‹ç‰¹ç‚¹
    â€¢ åŸºäº18ä¸ªä¸´åºŠé©±åŠ¨çš„é£é™©ç‰¹å¾
    â€¢ ä½¿ç”¨é€»è¾‘å›å½’ç®—æ³•ï¼ˆçº¿æ€§æ¨¡å‹ï¼‰
    â€¢ æ”¯æŒSHAPã€LIMEã€PDPå¯è§£é‡Šæ€§åˆ†æ
    """
    ax7.text(0.05, 0.95, summary, transform=ax7.transAxes, fontsize=12,
             verticalalignment='top', family='SimHei', 
             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))

    plt.tight_layout(rect=[0, 0, 1, 0.95])
    dashboard_path = os.path.join(VISUALIZATIONS_DIR, 'model_dashboard.png')
    plt.savefig(dashboard_path, dpi=300, bbox_inches='tight')
    print(f"âœ… ä»ªè¡¨æ¿å·²ä¿å­˜è‡³ {dashboard_path}")

    # 7. SHAP åˆ†æ
    print("\nğŸ” æ‰§è¡Œ SHAP åˆ†æ...")
    shap_path = shap_analysis(model_pipeline, X_train_scaled, X_test_scaled, feature_names, VISUALIZATIONS_DIR)

    # 8. LIME åˆ†æ
    print("\nğŸ” æ‰§è¡Œ LIME åˆ†æ...")
    lime_path = lime_analysis(model_pipeline, X_train_scaled, X_test_scaled, y_test, feature_names, VISUALIZATIONS_DIR)

    # 9. PDP åˆ†æ
    print("\nğŸ“ˆ æ‰§è¡Œ PDP åˆ†æ...")
    pdp_path = plot_pdp_analysis(model_pipeline, X_train_scaled, feature_names, VISUALIZATIONS_DIR)

    # 10. ç”ŸæˆæŠ¥å‘Š
    report = generate_report(metrics, best_thresh, best_f1, y_test, y_pred,
                             shap_path, lime_path, pdp_path)
    report_path = os.path.join(REPORTS_DIR, 'model_performance_report.md')
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"âœ… æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜è‡³ {report_path}")

    print("\nğŸ‰ æ‰€æœ‰åˆ†æå®Œæˆï¼")


if __name__ == "__main__":
    main()